{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surface realization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surface.grammar import Grammar\n",
    "from surface import converter\n",
    "from surface import utils\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we initialize the training and the test file to a variable, the files can be downloaded from the SRST 19 page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_FILE = \"data/en_tr_tr.conllu\"\n",
    "TRAIN_FILE = \"data/en_tr_sample.conllu\"\n",
    "TEST_FILE = \"data/en_tr_dev.conllu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we train the two static grammars (the first corresponds to the subgraphs from the ud trees, the second is the fallback grammar, where each rule is binary)\n",
    "\n",
    "Later, the dynamic grammars are generated from these ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_to_id, id_to_word = converter.build_dictionaries([TRAIN_FILE, TEST_FILE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if do_train:\n",
    "    grammar = Grammar()\n",
    "    grammar.train_subgraphs(TRAIN_FILE, word_to_id)\n",
    "    with open('grammar.bin', 'wb') as f:\n",
    "        pickle.dump(grammar, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not do_train:\n",
    "    with open('grammar.bin', 'rb') as f:\n",
    "        grammar = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to extract the graphs from the conll format (conversion from conll to isi), and the rules that use the <strong>lin</strong> feature.\n",
    "\n",
    "The rules are for incorporating the <strong>lin</strong> feature, so we can dynamically delete every rule the contradicts the linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules, _ = converter.extract_rules(TEST_FILE, word_to_id)\n",
    "graphs, _, id_graphs= converter.convert(TEST_FILE, word_to_id)\n",
    "#_, sentences, _ = converter.convert(TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload  \n",
    "reload(converter)\n",
    "reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run through the sentences and call the <strong>alto</strong> parser to generate the derivation and map the ud representation to string.\n",
    "\n",
    "The alto can be downloaded from [bitbucket](https://bitbucket.org/tclup/alto/downloads/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_grammar(sen_rules, grammar_fn, sen, binary=False):\n",
    "    with open(grammar_fn, 'w') as grammar_f:\n",
    "        grammar.generate_grammar(sen_rules, grammar_f, binary=binary)\n",
    "        grammar.generate_terminal_ids(sen, grammar_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_alto(timeout, input_fn, grammar_fn, output_fn):\n",
    "    !timeout $timeout java -Xmx32G -cp alto-2.3.6-all.jar de.up.ling.irtg.script.ParsingEvaluator $input_fn -g $grammar_fn -I ud -O string=toString -o $output_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_parse(id_graph, sen_rules, sen, prefix, timeout=5):\n",
    "    grammar_fn = f'{prefix}.irtg'\n",
    "    input_fn = f'{prefix}.input'\n",
    "    utils.set_parse(input_fn, id_graph)\n",
    "    output_fn = f'{prefix}.output'\n",
    "    try:\n",
    "        gen_grammar(sen_rules, grammar_fn, sen)\n",
    "        run_alto(timeout, input_fn, grammar_fn, output_fn)\n",
    "    except StopIteration:\n",
    "        print(f'sen {i} timed out, falling back to binary grammar')\n",
    "        gen_grammar(sen_rules, grammar_fn, sen, binary=True)\n",
    "        run_alto(timeout, input_fn, grammar_fn, output_fn)\n",
    "        \n",
    "    return utils.get_ids_from_parse(output_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll = utils.get_conll_from_file(TEST_FILE, word_to_id)\n",
    "pred_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_graphs[2]\n",
    "rules[2]\n",
    "conll[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dirname = 'gen'\n",
    "for i in range(4, len(rules)):\n",
    "# for i in range(0, 10):\n",
    "    print(i)\n",
    "    pred_ids.append(do_parse(id_graphs[i], rules[i], conll[i], f\"{dirname}/{i}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[t.word for t in sorted(conll[4], key=lambda tok: int(tok.misc.split('|')[-1].split('=')[-1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[t.pos for t in sorted(conll[4], key=lambda tok: int(tok.misc.split('|')[-1].split('=')[-1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.conllu\" , \"w\") as f:\n",
    "    for i in id_to_parse:\n",
    "        conll_f = id_to_parse[i][1]\n",
    "        for line in conll_f:\n",
    "            f.write(str(line) + \"\\t\")\n",
    "            f.write(\"\\t\".join(conll_f[line]))\n",
    "            f.write('\\n')\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.to_tokenized_output(\"test-results-inflected/\", \"tokenized_test_results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
