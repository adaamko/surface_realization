{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surface realization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surface.grammar import Grammar\n",
    "from surface import converter\n",
    "from surface import utils\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we initialize the training and the test file to a variable, the files can be downloaded from the SRST 19 page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = \"data/T1-train/en_ewt-ud-train.conllu\"\n",
    "TEST_FILE = \"data/T1-dev/en_ewt-ud-dev.conllu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we train the two static grammars (the first corresponds to the subgraphs from the ud trees, the second is the fallback grammar, where each rule is binary)\n",
    "\n",
    "Later, the dynamic grammars are generated from these ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_to_id, id_to_word = converter.build_dictionaries([TRAIN_FILE, TEST_FILE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "217128it [08:28, 427.27it/s] \n"
     ]
    }
   ],
   "source": [
    "grammar = Grammar()\n",
    "grammar.train_subgraphs(TRAIN_FILE, word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('grammar.bin', 'rb') as f:\n",
    "    grammar = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('grammar.bin', 'wb') as f:\n",
    "    pickle.dump(grammar, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to extract the graphs from the conll format (conversion from conll to isi), and the rules that use the <strong>lin</strong> feature.\n",
    "\n",
    "The rules are for incorporating the <strong>lin</strong> feature, so we can dynamically delete every rule the contradicts the linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules, _ = converter.extract_rules(TEST_FILE, word_to_id)\n",
    "graphs, _, id_graphs= converter.convert(TEST_FILE, word_to_id)\n",
    "#_, sentences, _ = converter.convert(TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll = converter.get_conll_from_file(TEST_FILE, word_to_id)\n",
    "id_to_parse = {}\n",
    "stops = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run through the sentences and call the <strong>alto</strong> parser to generate the derivation and map the ud representation to string.\n",
    "\n",
    "The alto can be downloaded from [bitbucket](https://bitbucket.org/tclup/alto/downloads/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_parse(id_graph, rules, conll, prefix, binary=False):\n",
    "    grammar_fn = f'{prefix}.irtg'\n",
    "    grammar_f = open(grammar_fn, 'w') \n",
    "    grammar.generate_grammar(rules, grammar_f, binary=binary)\n",
    "    grammar.generate_terminal_ids(conll, grammar_f)\n",
    "    grammar_f.close()\n",
    "    input_fn = f'{prefix}.input'\n",
    "    utils.set_parse(input_fn, id_graph)\n",
    "    output_fn = f'{prefix}.output'\n",
    "    !timeout 5 java -Xmx32G -cp alto-2.3.6-all.jar de.up.ling.irtg.script.ParsingEvaluator $input_fn -g $grammar_fn -I ud -O string=toString -o $output_fn\n",
    "    text_parse, conll_parse = utils.get_parse(output_fn, conll)\n",
    "    return text_parse, conll_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Processing gen/0.input (2 instances) ...\n",
      "1 [[WORD6879_1/WORD6879_1 -det-> WORD11_4/WORD11_4; W] 402 ms\n",
      "2 [[dummy_0/dummy_0]                                 ] 2 ms\n",
      "Done, total time: 541 ms\n",
      "1\n",
      "Processing gen/1.input (2 instances) ...\n",
      "1 [[WORD2500_3/WORD2500_3 -case-> WORD158_9/WORD158_9] stop iteration\n",
      "Processing gen/1.input (2 instances) ...\n",
      "1 [[WORD2500_3/WORD2500_3 -case-> WORD158_9/WORD158_9] 323 ms\n",
      "2 [[dummy_0/dummy_0]                                 ] 8 ms\n",
      "Done, total time: 416 ms\n",
      "2\n",
      "Processing gen/2.input (2 instances) ...\n",
      "1 [[WORD8990_11/WORD8990_11 -punct-> WORD23_2/WORD23_] 3.251s\n",
      "2 [[dummy_0/dummy_0]                                 ] 7 ms\n",
      "Done, total time: 3.322s\n",
      "3\n",
      "Processing gen/3.input (2 instances) ...\n",
      "1 [[WORD2213_1/WORD2213_1]                           ] 97 ms\n",
      "2 [[dummy_0/dummy_0]                                 ] 733 ?s\n",
      "Done, total time: 132 ms\n",
      "4\n",
      "Processing gen/4.input (2 instances) ...\n",
      "1 [[WORD8990_6/WORD8990_6 -punct-> WORD1_1/WORD1_1; W] stop iteration\n",
      "Processing gen/4.input (2 instances) ...\n",
      "1 [[WORD8990_6/WORD8990_6 -punct-> WORD1_1/WORD1_1; W] 346 ms\n",
      "2 [[dummy_0/dummy_0]                                 ] 3 ms\n",
      "Done, total time: 421 ms\n",
      "5\n",
      "Processing gen/5.input (2 instances) ...\n",
      "1 [[WORD1416_2/WORD1416_2 -det-> WORD11_8/WORD11_8; W] 611 ms\n",
      "2 [[dummy_0/dummy_0]                                 ] 6 ms\n",
      "Done, total time: 678 ms\n",
      "6\n",
      "Processing gen/6.input (2 instances) ...\n",
      "1 [[WORD108_1/WORD108_1 -cc-> WORD289_2/WORD289_2; WO] stop iteration\n",
      "Processing gen/6.input (2 instances) ...\n",
      "1 [[WORD108_1/WORD108_1 -cc-> WORD289_2/WORD289_2; WO] 544 ms\n",
      "2 [[dummy_0/dummy_0]                                 ] 3 ms\n",
      "Done, total time: 591 ms\n",
      "7\n",
      "Processing gen/7.input (2 instances) ...\n",
      "1 [[WORD1520_3/WORD1520_3 -aux-> WORD51_2/WORD51_2; W] 225 ms\n",
      "2 [[dummy_0/dummy_0]                                 ] 2 ms\n",
      "Done, total time: 263 ms\n",
      "8\n",
      "Processing gen/8.input (2 instances) ...\n",
      "1 [[WORD102_14/WORD102_14 -punct-> WORD1_5/WORD1_5; W] 626 ms\n",
      "2 [[dummy_0/dummy_0]                                 ] 7 ms\n",
      "Done, total time: 683 ms\n",
      "9\n",
      "Processing gen/9.input (2 instances) ...\n",
      "1 [[WORD30_3/WORD30_3 -flat_foreign-> WORD30_1/WORD30] stop iteration\n",
      "Processing gen/9.input (2 instances) ...\n",
      "1 [[WORD30_3/WORD30_3 -flat_foreign-> WORD30_1/WORD30] 332 ms\n",
      "2 [[dummy_0/dummy_0]                                 ] 2 ms\n",
      "Done, total time: 373 ms\n"
     ]
    }
   ],
   "source": [
    "dirname = 'gen'\n",
    "# for sen_id in range(0, len(rules)):\n",
    "for i in range(0, 10):\n",
    "    print(i)\n",
    "    try:\n",
    "        id_to_parse[i] = do_parse(id_graphs[i], rules[i], conll[i], f\"{dirname}/{i}\")\n",
    "    except StopIteration:\n",
    "        print(\"stop iteration\")\n",
    "        id_to_parse[i] = do_parse(id_graphs[i], rules[i], conll[i], f\"{dirname}/{i}\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.conllu\" , \"w\") as f:\n",
    "    for i in id_to_parse:\n",
    "        conll_f = id_to_parse[i][1]\n",
    "        for line in conll_f:\n",
    "            f.write(str(line) + \"\\t\")\n",
    "            f.write(\"\\t\".join(conll_f[line]))\n",
    "            f.write('\\n')\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.to_tokenized_output(\"test-results-inflected/\", \"tokenized_test_results/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
